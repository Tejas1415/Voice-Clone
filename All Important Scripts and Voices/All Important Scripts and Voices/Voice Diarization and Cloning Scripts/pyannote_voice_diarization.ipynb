{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face Pyannote Diarization Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install torch==2.0.0+cu118 -f https://download.pytorch.org/whl/cu118/torch_stable.html ## I dont think this is particularly useful\n",
    "pip install pyannote.audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\TejasReddy_od7b\\Downloads\\PersonalJob\\Personal\\voice_project\\voice\\lib\\site-packages\\pyannote\\audio\\core\\io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"soundfile\")\n",
      "c:\\Users\\TejasReddy_od7b\\Downloads\\PersonalJob\\Personal\\voice_project\\voice\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyannote.audio import Pipeline\n",
    "import torch\n",
    "import torchaudio\n",
    "import sounddevice as sd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "torchvision is not available - cannot save figures\n"
     ]
    }
   ],
   "source": [
    "## Get the Hugging Face Model for Voice Diarization\n",
    "pipeline = Pipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    use_auth_token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If GPU enabled, and we do\n",
    "```\n",
    "import torch\n",
    "pipeline.to(torch.device(\"cuda\"))\n",
    "```\n",
    "They claim it could process 1 hour conversation in 1.5 mins. \n",
    "Currently in CPU - it takes over 3 mins to process a 4 mins audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the pipeline on an audio file - took 3 mins and 5 seconds to run for 4 mins audio\n",
    "#diarization = pipeline(\"multiple_voice.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another way - took same 3 mins and 5 seconds to run a 4 mins audio file\n",
    "waveform, sample_rate = torchaudio.load(\"Debate example.wav\")\n",
    "diarization = pipeline({\"waveform\": waveform, \"sample_rate\": sample_rate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 00:00:00.008 -->  00:01:14.388] A SPEAKER_00\n",
      "[ 00:01:15.254 -->  00:01:17.088] B SPEAKER_01\n",
      "[ 00:01:17.292 -->  00:01:29.889] C SPEAKER_01\n",
      "[ 00:01:30.127 -->  00:01:40.144] D SPEAKER_01\n",
      "[ 00:01:40.636 -->  00:01:41.315] E SPEAKER_01\n",
      "[ 00:01:41.740 -->  00:01:55.271] F SPEAKER_01\n",
      "[ 00:01:55.475 -->  00:02:46.273] G SPEAKER_00\n",
      "[ 00:02:46.460 -->  00:02:52.928] H SPEAKER_00\n",
      "[ 00:02:53.285 -->  00:02:54.864] I SPEAKER_01\n",
      "[ 00:02:56.290 -->  00:02:58.904] J SPEAKER_01\n",
      "[ 00:02:59.193 -->  00:03:36.511] K SPEAKER_01\n",
      "[ 00:03:37.665 -->  00:03:38.938] L SPEAKER_01\n",
      "[ 00:03:39.006 -->  00:03:46.494] M SPEAKER_01\n",
      "[ 00:03:46.680 -->  00:03:48.361] N SPEAKER_01\n",
      "[ 00:03:48.887 -->  00:04:32.266] O SPEAKER_00\n",
      "[ 00:04:33.200 -->  00:04:35.628] P SPEAKER_01\n",
      "[ 00:04:36.188 -->  00:04:40.874] Q SPEAKER_01\n",
      "[ 00:04:41.315 -->  00:04:45.356] R SPEAKER_01\n",
      "[ 00:04:45.899 -->  00:04:50.025] S SPEAKER_01\n",
      "[ 00:04:50.365 -->  00:04:50.823] T SPEAKER_01\n",
      "[ 00:04:51.179 -->  00:04:57.071] U SPEAKER_01\n",
      "[ 00:04:57.478 -->  00:04:59.040] V SPEAKER_01\n",
      "[ 00:05:00.008 -->  00:05:07.631] W SPEAKER_01\n",
      "[ 00:05:08.039 -->  00:05:45.713] X SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "# print the diarization output\n",
    "print(diarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SPEAKER_00', 'SPEAKER_01']\n"
     ]
    }
   ],
   "source": [
    "# print speaker labels\n",
    "print(diarization.labels())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "### If we have to remove overlapped content\n",
    "print(diarization.get_overlap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now lets see if we can separate audio's based on the given timelines from pyannote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_wav(waveform, sample_rate, playback_duration=10):\n",
    "    # Define the duration to play (in seconds)\n",
    "    num_samples_to_play = min(len(waveform), int(playback_duration * sample_rate))\n",
    "    waveform_to_play = waveform[:num_samples_to_play]\n",
    "    \n",
    "    # Check if the waveform has only one channel, if yes, expand it to two channels\n",
    "    if waveform_to_play.ndim == 1:\n",
    "        waveform_to_play = np.expand_dims(waveform_to_play, axis=0)\n",
    "\n",
    "    # Play audio\n",
    "    sd.play(waveform_to_play.T, sample_rate)  # Transpose to match the expected shape (channels, samples)\n",
    "    sd.wait()\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarization_overlap_to_dataframe(diarization):\n",
    "    overlap_str = str(diarization.get_overlap())\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = overlap_str.split('\\n')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "\n",
    "    # Iterate over each line and extract data\n",
    "    for line in lines:\n",
    "        parts = line.split('] ')\n",
    "        time_parts = parts[0][2:].strip().split(' --> ')\n",
    "        start_times.append(time_parts[0].strip())\n",
    "        end_times.append(time_parts[1].replace(']','').strip())\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Start Time': start_times,\n",
    "        'End Time': end_times\n",
    "    })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diarization_to_dataframe(diarization):\n",
    "    # Convert diarization object to string\n",
    "    diarization_str = str(diarization)\n",
    "    \n",
    "    # Split the text into lines\n",
    "    lines = diarization_str.split('\\n')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    start_times = []\n",
    "    end_times = []\n",
    "    speaker_labels = []\n",
    "\n",
    "    # Iterate over each line and extract data\n",
    "    for line in lines:\n",
    "        parts = line.split('] ')\n",
    "        time_parts = parts[0][2:].strip().split(' --> ')\n",
    "        speaker_label = parts[1].strip()[2:].strip()\n",
    "        start_times.append(time_parts[0].strip())\n",
    "        end_times.append(time_parts[1].strip())\n",
    "        speaker_labels.append(speaker_label)\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Start Time': start_times,\n",
    "        'End Time': end_times,\n",
    "        'Speaker Label': speaker_labels\n",
    "    })\n",
    "\n",
    "    # Convert time columns to datetime\n",
    "    #df['Start Time'] = pd.to_datetime(df['Start Time'], infer_datetime_format=True)\n",
    "    #df['End Time'] = pd.to_datetime(df['End Time'], infer_datetime_format=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Speaker Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00.008</td>\n",
       "      <td>00:01:14.388</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:01:15.254</td>\n",
       "      <td>00:01:17.088</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:01:17.292</td>\n",
       "      <td>00:01:29.889</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:01:30.127</td>\n",
       "      <td>00:01:40.144</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:01:40.636</td>\n",
       "      <td>00:01:41.315</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00:01:41.740</td>\n",
       "      <td>00:01:55.271</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00:01:55.475</td>\n",
       "      <td>00:02:46.273</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00:02:46.460</td>\n",
       "      <td>00:02:52.928</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00:02:53.285</td>\n",
       "      <td>00:02:54.864</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00:02:56.290</td>\n",
       "      <td>00:02:58.904</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Start Time      End Time Speaker Label\n",
       "0  00:00:00.008  00:01:14.388    SPEAKER_00\n",
       "1  00:01:15.254  00:01:17.088    SPEAKER_01\n",
       "2  00:01:17.292  00:01:29.889    SPEAKER_01\n",
       "3  00:01:30.127  00:01:40.144    SPEAKER_01\n",
       "4  00:01:40.636  00:01:41.315    SPEAKER_01\n",
       "5  00:01:41.740  00:01:55.271    SPEAKER_01\n",
       "6  00:01:55.475  00:02:46.273    SPEAKER_00\n",
       "7  00:02:46.460  00:02:52.928    SPEAKER_00\n",
       "8  00:02:53.285  00:02:54.864    SPEAKER_01\n",
       "9  00:02:56.290  00:02:58.904    SPEAKER_01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_da = diarization_to_dataframe(diarization)\n",
    "df_da.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Check if there is an overlap between the voices, if yes then extract that and show to the user\n",
    "try:\n",
    "    df_da1 = diarization_overlap_to_dataframe(diarization)\n",
    "    df_da1['flag'] = 1\n",
    "    df_da1.head(10)\n",
    "except:\n",
    "    df_da1 = pd.DataFrame({\n",
    "        'Start Time': [],\n",
    "        'End Time': [],\n",
    "        'flag': []\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total_seconds(time_str):\n",
    "    time_str = time_str.strip()\n",
    "    start_time = datetime.strptime(time_str, '%H:%M:%S.%f').time()\n",
    "    time_delta = timedelta(hours=start_time.hour, minutes=start_time.minute, seconds=start_time.second, microseconds=start_time.microsecond)\n",
    "    total_secs = time_delta.total_seconds()\n",
    "    return total_secs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_speaker_audio(waveform, sample_rate, df):\n",
    "    speaker_waveforms = {}\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        start_time = row['Start Time']\n",
    "        end_time = row['End Time']\n",
    "        speaker_label = row['Speaker Label']\n",
    "        \n",
    "        # Convert start and end times to indices\n",
    "        start_index = int(calculate_total_seconds(start_time) * sample_rate)\n",
    "        end_index = int(calculate_total_seconds(end_time) * sample_rate)\n",
    "\n",
    "        # Extract segment from waveform\n",
    "        segment_waveform = waveform[:, start_index:end_index]\n",
    "        \n",
    "        # If the speaker label is not already in the dictionary, create a new entry\n",
    "        if speaker_label not in speaker_waveforms:\n",
    "            speaker_waveforms[speaker_label] = [segment_waveform]\n",
    "        else:\n",
    "            # If the speaker label already exists, append the segment waveform to the list\n",
    "            speaker_waveforms[speaker_label].append(segment_waveform)\n",
    "    \n",
    "    # Concatenate waveforms for each speaker\n",
    "    concatenated_waveforms = {}\n",
    "    for speaker_label, waveforms_list in speaker_waveforms.items():\n",
    "        concatenated_waveforms[speaker_label] = torch.cat(waveforms_list, dim=1)\n",
    "    \n",
    "    return concatenated_waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Start Time</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Speaker Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00:00:00.008</td>\n",
       "      <td>00:01:14.388</td>\n",
       "      <td>SPEAKER_00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00:01:15.254</td>\n",
       "      <td>00:01:17.088</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00:01:17.292</td>\n",
       "      <td>00:01:29.889</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00:01:30.127</td>\n",
       "      <td>00:01:40.144</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00:01:40.636</td>\n",
       "      <td>00:01:41.315</td>\n",
       "      <td>SPEAKER_01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Start Time      End Time Speaker Label\n",
       "0  00:00:00.008  00:01:14.388    SPEAKER_00\n",
       "1  00:01:15.254  00:01:17.088    SPEAKER_01\n",
       "2  00:01:17.292  00:01:29.889    SPEAKER_01\n",
       "3  00:01:30.127  00:01:40.144    SPEAKER_01\n",
       "4  00:01:40.636  00:01:41.315    SPEAKER_01"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### IF we decide to delete the overlapped content from the voices\n",
    "df_merge = df_da.merge(df_da1, on = ['Start Time', 'End Time'], how = 'left')\n",
    "df_non_overlap = df_merge[df_merge['flag'].isnull()]\n",
    "df_non_overlap = df_non_overlap.drop(['flag'], axis = 1)\n",
    "df_non_overlap.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If we need to use full audio including overlaps\n",
    "#speaker_waveforms = extract_speaker_audio(waveform, sample_rate, df_da)\n",
    "\n",
    "## If we don't want to use overlapped audio'\n",
    "speaker_waveforms = extract_speaker_audio(waveform, sample_rate, df_non_overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SPEAKER_00': tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0026, -0.0026, -0.0024],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ..., -0.0026, -0.0026, -0.0024]]),\n",
       " 'SPEAKER_01': tensor([[-0.0013, -0.0010, -0.0016,  ..., -0.0044, -0.0048, -0.0051],\n",
       "         [-0.0013, -0.0010, -0.0016,  ..., -0.0044, -0.0048, -0.0051]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speaker_waveforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_wav(waveform, sample_rate, playback_duration=30):\n",
    "    # Define the duration to play (in seconds)\n",
    "    num_samples_to_play =  int(playback_duration * sample_rate)\n",
    "    waveform_to_play = waveform[:, :num_samples_to_play]\n",
    "    \n",
    "    # Convert tensor to numpy array\n",
    "    waveform_np = waveform_to_play.numpy()\n",
    "    \n",
    "    # Check if the waveform has only one channel, if yes, expand it to two channels\n",
    "    if waveform_np.ndim == 1:\n",
    "        waveform_np = waveform_np.reshape(1, -1)\n",
    "\n",
    "    # Play audio\n",
    "    sd.play(waveform_np.T, sample_rate)  # Transpose to match the expected shape (channels, samples)\n",
    "    sd.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing audio for speaker SPEAKER_00\n"
     ]
    }
   ],
   "source": [
    "# Play the audio of each speaker\n",
    "for speaker_label, waveform in speaker_waveforms.items():\n",
    "    if speaker_label == 'SPEAKER_00':\n",
    "        print(f\"Playing audio for speaker {speaker_label}\")\n",
    "        play_wav(waveform, sample_rate, playback_duration=30)\n",
    "\n",
    "    ## Save the files\n",
    "    torchaudio.save(\"debate_{}.wav\".format(speaker_label), waveform, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('voice': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d803aa8052b7b977dfa4302a7282d7b4d58e11e17159b87250602a3fd8423c4f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
